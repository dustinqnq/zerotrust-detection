#!/usr/bin/env python3
"""
å¢å¼ºé›¶ä¿¡ä»»IDSè®­ç»ƒè„šæœ¬
ä½¿ç”¨IoT-23æ•°æ®é›†è®­ç»ƒæ”¹è¿›çš„ä¸‰é˜¶æ®µæ¨¡å‹
"""

import sys
import os
sys.path.append('../src')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from enhanced_autoencoder_ids import EnhancedZeroTrustIDS
from processors.iot23_processor import IoT23Processor
import torch
from pathlib import Path
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
import seaborn as sns
import itertools
from sklearn.preprocessing import LabelEncoder

def prepare_iot23_data():
    """å‡†å¤‡IoT-23æ•°æ®é›†å¹¶å¤„ç†æ•°æ®ä¸å¹³è¡¡é—®é¢˜"""
    print("\n=== å¼€å§‹åŠ è½½IoT-23æ•°æ®é›† ===")
    print("1. æŸ¥æ‰¾æ•°æ®æ–‡ä»¶...")
    
    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    data_dir = Path('../data/iot-23')  # ä¿®æ”¹æ•°æ®ç›®å½•è·¯å¾„
    print(f"æœç´¢ç›®å½•: {data_dir.absolute()}")
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨çš„æ•è·æ–‡ä»¶
    capture_dirs = list(data_dir.glob('CTU-IoT-Malware-Capture-*-*'))
    data_files = []
    for capture_dir in capture_dirs:
        labeled_files = list(capture_dir.glob('**/*.labeled'))
        data_files.extend(labeled_files)
    
    if not data_files:
        print("âŒ æœªæ‰¾åˆ°IoT-23æ•°æ®æ–‡ä»¶ï¼Œè¯·ç¡®ä¿æ•°æ®æ–‡ä»¶åœ¨æ­£ç¡®çš„ç›®å½•ä¸‹")
        print(f"å½“å‰æœç´¢ç›®å½•: {data_dir.absolute()}")
        return None, None, None
    
    print(f"âœ“ æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶:")
    for f in data_files:
        print(f"  - {f.name} ({f.stat().st_size / (1024*1024):.1f} MB)")
    
    # å¤„ç†å¤šä¸ªæ–‡ä»¶å¹¶åˆå¹¶
    all_features = []
    all_labels = []
    all_detailed_labels = []
    
    for file_idx, file_path in enumerate(data_files[:3]):  # é™åˆ¶å¤„ç†å‰3ä¸ªæ–‡ä»¶ä»¥é¿å…å†…å­˜é—®é¢˜
        print(f"\n2.{file_idx+1} æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path.name}")
        print("   å¼€å§‹è¯»å–æ•°æ®...")
        
        # è¯»å–å’Œè§£ææ•°æ®
        features_list = []
        labels_list = []
        detailed_labels_list = []
        processed_lines = 0
        total_lines = 20000  # æ¯ä¸ªæ–‡ä»¶é™åˆ¶å¤„ç†çš„è¡Œæ•°
        
        try:
            with open(file_path, 'r') as f:
                for i, line in enumerate(f):
                    if i > total_lines:
                        break
                        
                    if i % 2000 == 0 and i > 0:  # æ¯å¤„ç†2000è¡Œæ‰“å°ä¸€æ¬¡è¿›åº¦
                        print(f"   å·²å¤„ç† {i} è¡Œ...")
                        
                    line = line.strip()
                    if line and not line.startswith('#'):
                        try:
                            fields = line.split('\t')  # ä½¿ç”¨tabåˆ†éš”ç¬¦
                            
                            if len(fields) >= 21:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å­—æ®µ
                                # æå–æ•°å€¼ç‰¹å¾ï¼ˆè·³è¿‡å‰é¢çš„éæ•°å€¼å­—æ®µï¼Œä»æ—¶é—´æˆ³ç­‰å¼€å§‹ï¼‰
                                numeric_features = []
                                
                                # ä»ç¬¬1åˆ—å¼€å§‹ï¼ˆæ—¶é—´æˆ³ï¼‰ï¼Œé€‰æ‹©ä¸€äº›æ•°å€¼å­—æ®µ
                                numeric_indices = [0, 3, 4, 5, 8, 9, 14, 16, 17, 18, 19]  # é€‰æ‹©æ•°å€¼å­—æ®µçš„ç´¢å¼•
                                
                                for idx in numeric_indices:
                                    if idx < len(fields):
                                        try:
                                            # å¤„ç†ç‰¹æ®Šå€¼
                                            val_str = fields[idx].strip()
                                            if val_str == '-' or val_str == '(empty)' or val_str == '':
                                                val = 0.0
                                            else:
                                                val = float(val_str)
                                            numeric_features.append(val)
                                        except:
                                            numeric_features.append(0.0)
                                    else:
                                        numeric_features.append(0.0)
                                
                                # ç¡®ä¿ç‰¹å¾ç»´åº¦ä¸€è‡´ï¼ˆæ‰©å±•åˆ°20ç»´ï¼‰
                                while len(numeric_features) < 20:
                                    numeric_features.append(0.0)
                                
                                # é™åˆ¶åˆ°20ç»´
                                numeric_features = numeric_features[:20]
                                    
                                # è·å–æ ‡ç­¾ - æœ€åä¸€åˆ—åŒ…å«å¤åˆæ ‡ç­¾ä¿¡æ¯
                                if len(fields) >= 21:
                                    label_info = fields[-1].strip()
                                    # è§£æå¤åˆæ ‡ç­¾ï¼š"(empty)   Malicious   PartOfAHorizontalPortScan"
                                    label_parts = label_info.split()
                                    
                                    if len(label_parts) >= 2:
                                        basic_label = label_parts[1].strip().lower()  # Malicious/Benign
                                        detailed_label = label_parts[2] if len(label_parts) > 2 else 'Unknown'
                                        
                                        # åªä¿ç•™æœ‰æ•ˆçš„æ ‡ç­¾
                                        if basic_label in ['benign', 'malicious']:
                                            features_list.append(numeric_features)
                                            labels_list.append(basic_label)
                                            detailed_labels_list.append(detailed_label)
                                            processed_lines += 1
                                
                        except Exception as e:
                            continue  # é™é»˜è·³è¿‡è§£æå¤±è´¥çš„è¡Œ
                            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶è¯»å–å‡ºé”™: {str(e)}")
            continue
        
        print(f"   æ–‡ä»¶ {file_path.name} å¤„ç†å®Œæˆ: {processed_lines} è¡Œ")
        
        # åˆå¹¶åˆ°æ€»åˆ—è¡¨
        all_features.extend(features_list)
        all_labels.extend(labels_list)
        all_detailed_labels.extend(detailed_labels_list)
    
    print(f"\n3. å¤šæ–‡ä»¶æ•°æ®åˆå¹¶å®Œæˆ:")
    print(f"   - æ€»æœ‰æ•ˆæ ·æœ¬æ•°: {len(all_features)}")
    
    if not all_features:
        print("âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆæ•°æ®")
        return None, None, None
    
    # è½¬æ¢ä¸ºDataFrameå’Œæ•°ç»„
    print("\n4. è½¬æ¢æ•°æ®æ ¼å¼...")
    features_df = pd.DataFrame(all_features)
    labels_array = np.array(all_labels)
    detailed_labels_array = np.array(all_detailed_labels)
    
    # åˆ›å»ºå®Œæ•´çš„æ•°æ®é›†DataFrame
    dataset_df = features_df.copy()
    dataset_df['basic_label'] = labels_array
    dataset_df['detailed_label'] = detailed_labels_array
    
    print(f"\n5. åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"   ç‰¹å¾å½¢çŠ¶: {features_df.shape}")
    print(f"   åŸºæœ¬æ ‡ç­¾åˆ†å¸ƒ:")
    unique_labels, counts = np.unique(labels_array, return_counts=True)
    for label, count in zip(unique_labels, counts):
        percentage = count / len(labels_array) * 100
        print(f"   - {label}: {count} ({percentage:.1f}%)")
    
    print(f"\n   è¯¦ç»†æ ‡ç­¾åˆ†å¸ƒ (Top 10):")
    detailed_counts = pd.Series(detailed_labels_array).value_counts()
    for label, count in detailed_counts.head(10).items():
        percentage = count / len(detailed_labels_array) * 100
        print(f"   - {label}: {count} ({percentage:.1f}%)")
    
    # å®ç°åˆ†å±‚é‡‡æ ·
    print("\n6. å®æ–½åˆ†å±‚é‡‡æ ·ç­–ç•¥...")
    
    # åˆ†ç¦»è‰¯æ€§å’Œæ¶æ„æ ·æœ¬
    benign_mask = dataset_df['basic_label'] == 'benign'
    malicious_mask = dataset_df['basic_label'] == 'malicious'
    
    benign_data = dataset_df[benign_mask]
    malicious_data = dataset_df[malicious_mask]
    
    print(f"   è‰¯æ€§æ ·æœ¬æ•°é‡: {len(benign_data)}")
    print(f"   æ¶æ„æ ·æœ¬æ•°é‡: {len(malicious_data)}")
    
    # å¯¹æ¶æ„æ ·æœ¬æŒ‰è¯¦ç»†æ ‡ç­¾è¿›è¡Œåˆ†å±‚é‡‡æ ·
    malicious_groups = malicious_data.groupby('detailed_label')
    stratified_malicious = []
    
    print("\n   æ¶æ„æ ·æœ¬åˆ†å±‚é‡‡æ ·:")
    for detailed_label, group in malicious_groups:
        if len(group) >= 50:  # å¦‚æœæ ·æœ¬å……è¶³ï¼Œé‡‡æ ·50ä¸ª
            sampled = group.sample(n=50, random_state=42)
            print(f"   - {detailed_label}: é‡‡æ · 50 ä¸ª (åŸæœ‰ {len(group)} ä¸ª)")
        elif len(group) >= 10:  # å¦‚æœæ ·æœ¬è¾ƒå°‘ä½†ä¸æ˜¯å¤ªå°‘ï¼Œå…¨éƒ¨ä½¿ç”¨
            sampled = group
            print(f"   - {detailed_label}: ä½¿ç”¨å…¨éƒ¨ {len(group)} ä¸ª")
        else:  # æ ·æœ¬å¤ªå°‘ï¼Œä½¿ç”¨è¿‡é‡‡æ ·
            sampled = group.sample(n=10, replace=True, random_state=42)
            print(f"   - {detailed_label}: è¿‡é‡‡æ ·åˆ° 10 ä¸ª (åŸæœ‰ {len(group)} ä¸ª)")
        
        stratified_malicious.append(sampled)
    
    # åˆå¹¶åˆ†å±‚é‡‡æ ·çš„æ¶æ„æ ·æœ¬
    stratified_malicious_df = pd.concat(stratified_malicious, ignore_index=True)
    
    # å¯¹è‰¯æ€§æ ·æœ¬è¿›è¡Œé‡‡æ ·ï¼Œä½¿æ•°æ®ç›¸å¯¹å¹³è¡¡
    target_benign_size = min(len(stratified_malicious_df), len(benign_data), 500)
    if len(benign_data) >= target_benign_size:
        sampled_benign_df = benign_data.sample(n=target_benign_size, random_state=42)
    else:
        sampled_benign_df = benign_data.sample(n=target_benign_size, replace=True, random_state=42)
    
    print(f"   è‰¯æ€§æ ·æœ¬é‡‡æ ·: {len(sampled_benign_df)} ä¸ª")
    
    # åˆå¹¶æœ€ç»ˆæ•°æ®é›†
    final_dataset = pd.concat([sampled_benign_df, stratified_malicious_df], ignore_index=True)
    
    # éšæœºæ‰“ä¹±æ•°æ®
    final_dataset = final_dataset.sample(frac=1, random_state=42).reset_index(drop=True)
    
    print(f"\n7. åˆ†å±‚é‡‡æ ·åæ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(final_dataset)}")
    print(f"   åŸºæœ¬æ ‡ç­¾åˆ†å¸ƒ:")
    basic_counts = final_dataset['basic_label'].value_counts()
    for label, count in basic_counts.items():
        percentage = count / len(final_dataset) * 100
        print(f"   - {label}: {count} ({percentage:.1f}%)")
    
    print(f"\n   è¯¦ç»†æ ‡ç­¾åˆ†å¸ƒ:")
    detailed_counts = final_dataset['detailed_label'].value_counts()
    for label, count in detailed_counts.items():
        percentage = count / len(final_dataset) * 100
        print(f"   - {label}: {count} ({percentage:.1f}%)")
    
    # æå–ç‰¹å¾å’Œæ ‡ç­¾
    balanced_features = final_dataset.iloc[:, :-2]  # é™¤äº†æœ€åä¸¤åˆ—æ ‡ç­¾å¤–çš„æ‰€æœ‰ç‰¹å¾
    balanced_labels = final_dataset['basic_label'].values
    
    # ç‰¹å¾é¢„å¤„ç†
    print("\n8. ç‰¹å¾é¢„å¤„ç†...")
    balanced_features = balanced_features.fillna(0)
    balanced_features = balanced_features.replace([np.inf, -np.inf], 0)
    
    print("âœ… æ•°æ®å‡†å¤‡å®Œæˆ!")
    print(f"   æœ€ç»ˆç‰¹å¾å½¢çŠ¶: {balanced_features.shape}")
    print(f"   åŒ…å«çš„æ”»å‡»å˜ç§æ•°é‡: {final_dataset['detailed_label'].nunique()}")
    
    return balanced_features, balanced_labels, final_dataset

def create_synthetic_attack_labels(labels, features, detailed_labels_df=None):
    """åˆ›å»ºåˆæˆçš„æ”»å‡»æ ‡ç­¾ç”¨äºè®­ç»ƒ"""
    print("=== åˆ›å»ºè®­ç»ƒæ ‡ç­¾ ===")
    
    # åŸºç¡€äºŒåˆ†ç±»æ ‡ç­¾ (0: æ­£å¸¸, 1: æ”»å‡»)
    binary_labels = np.array(['Benign' if 'benign' in str(label).lower() else 'Attack' for label in labels])
    
    # å¦‚æœæœ‰è¯¦ç»†æ ‡ç­¾ä¿¡æ¯ï¼Œä½¿ç”¨å®ƒæ¥åˆ›å»ºæ›´å‡†ç¡®çš„åˆ†ç±»
    if detailed_labels_df is not None and 'detailed_label' in detailed_labels_df.columns:
        detailed_labels = detailed_labels_df['detailed_label'].values
        print(f"ä½¿ç”¨è¯¦ç»†æ ‡ç­¾ä¿¡æ¯ï¼Œå…± {len(np.unique(detailed_labels))} ç§æ”»å‡»å˜ç§")
        
        # åˆ›å»ºä¸»æ”»å‡»ç±»å‹æ ‡ç­¾ï¼ˆåŸºäºè¯¦ç»†æ ‡ç­¾ï¼‰
        attack_types = []
        for detail_label in detailed_labels:
            detail_str = str(detail_label).lower()
            if 'benign' in detail_str or detail_str == '-':
                attack_types.append('Benign')
            elif any(keyword in detail_str for keyword in ['ddos', 'flood']):
                attack_types.append('DDoS')
            elif any(keyword in detail_str for keyword in ['mirai', 'okiru']):
                attack_types.append('Mirai')
            elif any(keyword in detail_str for keyword in ['scan', 'portscan', 'horizontalportscan']):
                attack_types.append('PortScan')
            elif any(keyword in detail_str for keyword in ['bot', 'torii', 'muhstik']):
                attack_types.append('Botnet')
            elif any(keyword in detail_str for keyword in ['c&c', 'cc', 'heartbeat']):
                attack_types.append('C&C')
            elif any(keyword in detail_str for keyword in ['attack', 'malicious']):
                attack_types.append('Attack')
            else:
                attack_types.append('Other')
        
        # åˆ›å»ºå­ç±»å‹æ ‡ç­¾ï¼ˆåŸºäºè¯¦ç»†æ ‡ç­¾çš„å…·ä½“ç±»å‹ï¼‰
        sub_types = []
        for detail_label in detailed_labels:
            detail_str = str(detail_label).lower()
            if 'benign' in detail_str or detail_str == '-':
                sub_types.append('Normal')
            elif 'tcp' in detail_str:
                sub_types.append('TCP_Attack')
            elif 'udp' in detail_str:
                sub_types.append('UDP_Attack')
            elif 'http' in detail_str:
                sub_types.append('HTTP_Attack')
            elif any(keyword in detail_str for keyword in ['c&c', 'cc', 'heartbeat']):
                sub_types.append('CC_Attack')
            elif any(keyword in detail_str for keyword in ['scan', 'portscan']):
                sub_types.append('Scan_Attack')
            elif 'partofhorizontalportscan' in detail_str.replace(' ', ''):
                sub_types.append('Horizontal_Scan')
            elif 'filedownload' in detail_str:
                sub_types.append('FileDownload_Attack')
            elif any(keyword in detail_str for keyword in ['mirai', 'okiru', 'muhstik']):
                sub_types.append('Botnet_Variant')
            else:
                sub_types.append('Generic_Attack')
                
    else:
        # ä¼ ç»Ÿçš„æ ‡ç­¾åˆ›å»ºæ–¹æ³•ï¼ˆå¦‚æœæ²¡æœ‰è¯¦ç»†æ ‡ç­¾ä¿¡æ¯ï¼‰
        print("ä½¿ç”¨åŸºç¡€æ ‡ç­¾ä¿¡æ¯åˆ›å»ºæ”»å‡»åˆ†ç±»")
        
        # åˆ›å»ºä¸»æ”»å‡»ç±»å‹æ ‡ç­¾
        attack_types = []
        for label in labels:
            label_str = str(label).lower()
            if 'benign' in label_str:
                attack_types.append('Benign')
            elif 'ddos' in label_str:
                attack_types.append('DDoS')
            elif 'mirai' in label_str or 'okiru' in label_str:
                attack_types.append('Mirai')
            elif 'scan' in label_str:
                attack_types.append('PortScan')
            elif 'bot' in label_str:
                attack_types.append('Botnet')
            elif 'c&c' in label_str:
                attack_types.append('C&C')
            else:
                attack_types.append('Other')
        
        # åˆ›å»ºå­ç±»å‹æ ‡ç­¾
        sub_types = []
        for label in labels:
            label_str = str(label).lower()
            if 'benign' in label_str:
                sub_types.append('Normal')
            elif 'tcp' in label_str:
                sub_types.append('TCP_Attack')
            elif 'udp' in label_str:
                sub_types.append('UDP_Attack')
            elif 'http' in label_str:
                sub_types.append('HTTP_Attack')
            elif 'c&c' in label_str:
                sub_types.append('CC_Attack')
            elif 'scan' in label_str:
                sub_types.append('Scan_Attack')
            else:
                sub_types.append('Generic_Attack')
    
    # è½¬æ¢ä¸ºæ•°å€¼æ ‡ç­¾
    # äºŒåˆ†ç±»ç¼–ç 
    le_binary = LabelEncoder()
    binary_encoded = le_binary.fit_transform(binary_labels)
    
    # ä¸»ç±»å‹ç¼–ç 
    le_attack = LabelEncoder()
    attack_encoded = le_attack.fit_transform(attack_types)
    
    # å­ç±»å‹ç¼–ç 
    le_sub = LabelEncoder()
    sub_encoded = le_sub.fit_transform(sub_types)
    
    print(f"æ ‡ç­¾åˆ›å»ºå®Œæˆ:")
    print(f"  äºŒåˆ†ç±»æ ‡ç­¾: {len(np.unique(binary_labels))} ç±»")
    print(f"  ä¸»æ”»å‡»ç±»å‹: {len(np.unique(attack_types))} ç±» - {np.unique(attack_types)}")
    print(f"  å­æ”»å‡»ç±»å‹: {len(np.unique(sub_types))} ç±» - {np.unique(sub_types)}")
    
    # è®¡ç®—åˆ†å¸ƒç»Ÿè®¡
    print(f"\næ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡:")
    print("ä¸»æ”»å‡»ç±»å‹åˆ†å¸ƒ:")
    for attack_type in np.unique(attack_types):
        count = np.sum(np.array(attack_types) == attack_type)
        percentage = count / len(attack_types) * 100
        print(f"  - {attack_type}: {count} ({percentage:.1f}%)")
    
    print("å­æ”»å‡»ç±»å‹åˆ†å¸ƒ:")
    for sub_type in np.unique(sub_types):
        count = np.sum(np.array(sub_types) == sub_type)
        percentage = count / len(sub_types) * 100
        print(f"  - {sub_type}: {count} ({percentage:.1f}%)")
    
    return {
        'binary': binary_encoded,
        'attack_type': attack_encoded,
        'sub_type': sub_encoded,
        'binary_classes': le_binary.classes_,
        'attack_classes': le_attack.classes_,
        'sub_classes': le_sub.classes_
    }

def train_enhanced_model():
    """è®­ç»ƒå¢å¼ºæ¨¡å‹"""
    print("\nğŸš€ å¼€å§‹è®­ç»ƒå¢å¼ºé›¶ä¿¡ä»»IDSæ¨¡å‹")
    print("=" * 50)
    
    # 1. åŠ è½½æ•°æ®
    print("\n[1/10] åŠ è½½æ•°æ®...")
    features, labels, dataset_df = prepare_iot23_data()
    if features is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
        return
    
    # 2. åˆ›å»ºæ ‡ç­¾
    print("\n[2/10] åˆ›å»ºè®­ç»ƒæ ‡ç­¾...")
    attack_labels = create_synthetic_attack_labels(labels, features, dataset_df)
    
    # 3. åˆ†å‰²æ•°æ®é›†
    print("\n[3/10] åˆ†å‰²æ•°æ®é›†...")
    X_train, X_test, y_bin_train, y_bin_test, y_att_train, y_att_test, y_sub_train, y_sub_test = train_test_split(
        features, attack_labels['binary'], attack_labels['attack_type'], attack_labels['sub_type'], 
        test_size=0.2, random_state=42, stratify=attack_labels['binary']
    )
    
    X_train, X_val, y_bin_train, y_bin_val, y_att_train, y_att_val, y_sub_train, y_sub_val = train_test_split(
        X_train, y_bin_train, y_att_train, y_sub_train,
        test_size=0.2, random_state=42, stratify=y_bin_train
    )
    
    print(f"   è®­ç»ƒé›†: {X_train.shape}")
    print(f"   éªŒè¯é›†: {X_val.shape}")
    print(f"   æµ‹è¯•é›†: {X_test.shape}")
    
    # 4. åˆå§‹åŒ–æ¨¡å‹
    print("\n[4/10] åˆå§‹åŒ–å¢å¼ºæ¨¡å‹...")
    input_dim = features.shape[1]
    
    # è®¡ç®—ç±»åˆ«æ•°é‡
    num_binary_classes = len(attack_labels['binary_classes'])
    num_attack_classes = len(attack_labels['attack_classes'])
    num_sub_classes = len(attack_labels['sub_classes'])
    
    print(f"   äºŒåˆ†ç±»ç±»åˆ«æ•°: {num_binary_classes}")
    print(f"   ä¸»æ”»å‡»ç±»åˆ«æ•°: {num_attack_classes}")
    print(f"   å­æ”»å‡»ç±»åˆ«æ•°: {num_sub_classes}")
    
    model = EnhancedZeroTrustIDS(input_dim=input_dim)
    
    # 5. æ•°æ®é¢„å¤„ç†
    print("\n[5/10] æ•°æ®é¢„å¤„ç†...")
    print("   - æ ‡å‡†åŒ–ç‰¹å¾...")
    
    # å°†ç¼–ç åçš„æ ‡ç­¾è½¬æ¢ä¸ºå­—ç¬¦ä¸²å½¢å¼ä»¥ä¾¿æ¨¡å‹å¤„ç†
    y_bin_train_str = attack_labels['binary_classes'][y_bin_train]
    y_att_train_str = attack_labels['attack_classes'][y_att_train]
    y_sub_train_str = attack_labels['sub_classes'][y_sub_train]
    
    X_train_scaled, y_bin_train_enc, y_att_train_enc, y_sub_train_enc = model.prepare_data(
        X_train, y_bin_train_str, y_att_train_str, y_sub_train_str
    )
    
    print("   - å¤„ç†éªŒè¯é›†...")
    X_val_scaled = model.scaler.transform(X_val)
    
    # å¯¹éªŒè¯é›†æ ‡ç­¾è¿›è¡Œç›¸åŒçš„è½¬æ¢
    y_bin_val_str = attack_labels['binary_classes'][y_bin_val]
    y_att_val_str = attack_labels['attack_classes'][y_att_val]
    y_sub_val_str = attack_labels['sub_classes'][y_sub_val]
    
    y_bin_val_enc = model.label_encoders['binary'].transform(y_bin_val_str)
    y_att_val_enc = model.label_encoders['multi_stage1'].transform(y_att_val_str)
    y_sub_val_enc = model.label_encoders['multi_stage2'].transform(y_sub_val_str)
    
    print("   - å¤„ç†æµ‹è¯•é›†...")
    X_test_scaled = model.scaler.transform(X_test)
    
    # å¯¹æµ‹è¯•é›†æ ‡ç­¾è¿›è¡Œç›¸åŒçš„è½¬æ¢
    y_bin_test_str = attack_labels['binary_classes'][y_bin_test]
    y_bin_test_enc = model.label_encoders['binary'].transform(y_bin_test_str)
    
    # 6. è®­ç»ƒé˜¶æ®µ1
    print("\n[6/10] ğŸ”¥ Training Stage 1 - Shallow Classifier")
    print("   Starting training...")
    model.train_stage1(
        X_train_scaled, y_bin_train_enc, y_att_train_enc,
        X_val_scaled, y_bin_val_enc, y_att_val_enc,
        epochs=30  # å¢åŠ è®­ç»ƒè½®æ•°
    )
    
    # 7. è®­ç»ƒé˜¶æ®µ2
    print("\n[7/10] ğŸ”¥ è®­ç»ƒé˜¶æ®µ2 - æ·±å±‚åˆ†ç±»å™¨")
    print("   å¼€å§‹è®­ç»ƒ...")
    model.train_stage2(
        X_train_scaled, y_bin_train_enc, y_sub_train_enc,
        X_val_scaled, y_bin_val_enc, y_sub_val_enc,
        epochs=30  # å¢åŠ è®­ç»ƒè½®æ•°
    )
    
    # 8. å‡†å¤‡è‡ªç¼–ç å™¨è®­ç»ƒæ•°æ®
    print("\n[8/10] Preparing Autoencoder Training Data...")
    normal_mask = y_bin_train_enc == 0
    X_normal = X_train_scaled[normal_mask]
    
    print(f"   Normal traffic samples: {len(X_normal)} / {len(X_train_scaled)}")
    print(f"   Ratio: {len(X_normal)/len(X_train_scaled)*100:.1f}%")
    
    if len(X_normal) < 100:
        print("\nâš ï¸  Warning: Too few normal samples, using mixed training strategy")
        print("   - Using all samples with weighted normal samples")
        sample_weights = np.ones(len(X_train_scaled))
        sample_weights[normal_mask] = 3.0
        X_normal = X_train_scaled
        print(f"   - Normal sample weight=3.0, anomaly sample weight=1.0")
    elif len(X_normal) < 500:
        print("\nâš ï¸  Limited normal samples, might affect performance")
    else:
        print("\nâœ“ Sufficient normal samples")
    
    # 9. è®­ç»ƒé˜¶æ®µ3 - è‡ªç¼–ç å™¨
    print("\n[9/10] ğŸ”¥ Training Stage 3 - Autoencoder")
    if len(X_normal) >= 100:
        print("   Starting training...")
        ae_losses = model.train_autoencoder(X_normal, epochs=50)  # å¢åŠ è®­ç»ƒè½®æ•°
    else:
        print("âŒ Skipping autoencoder training: insufficient normal samples")
        ae_losses = ([], [])
    
    # 10. æ¨¡å‹è¯„ä¼°
    print("\n[10/10] ğŸ“Š æ¨¡å‹è¯„ä¼°")
    print("   è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
    results = model.evaluate(X_test_scaled, y_bin_test_enc)
    
    # æ•°æ®å¹³è¡¡æ€§åˆ†æ
    print("\n=== æ•°æ®é›†å¹³è¡¡æ€§åˆ†æ ===")
    test_benign_ratio = np.mean(y_bin_test_enc == 0) * 100
    test_malicious_ratio = np.mean(y_bin_test_enc == 1) * 100
    
    print(f"æµ‹è¯•é›†åˆ†å¸ƒ:")
    print(f"- è‰¯æ€§æ ·æœ¬: {test_benign_ratio:.1f}%")
    print(f"- æ¶æ„æ ·æœ¬: {test_malicious_ratio:.1f}%")
    
    # è¯„ä¼°æ•°æ®å¹³è¡¡æ€§
    imbalance_ratio = max(test_benign_ratio, test_malicious_ratio) / min(test_benign_ratio, test_malicious_ratio)
    
    if imbalance_ratio > 10:
        print(f"\nâš ï¸  æåº¦ä¸å¹³è¡¡ (æ¯”ä¾‹ {imbalance_ratio:.1f}:1)")
    elif imbalance_ratio > 3:
        print(f"\nâš ï¸  è½»åº¦ä¸å¹³è¡¡ (æ¯”ä¾‹ {imbalance_ratio:.1f}:1)")
    else:
        print(f"\nâœ“ æ•°æ®ç›¸å¯¹å¹³è¡¡ (æ¯”ä¾‹ {imbalance_ratio:.1f}:1)")
    
    # è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
    predictions = model.predict_binary(X_test_scaled)
    precision, recall, f1, _ = precision_recall_fscore_support(y_bin_test_enc, predictions, average=None)
    
    print("\n=== è¯¦ç»†æ€§èƒ½æŒ‡æ ‡ ===")
    print("è‰¯æ€§ç±»:")
    print(f"- ç²¾ç¡®åº¦: {precision[0]:.3f}")
    print(f"- å¬å›ç‡: {recall[0]:.3f}")
    print(f"- F1åˆ†æ•°: {f1[0]:.3f}")
    
    print("\næ¶æ„ç±»:")
    print(f"- ç²¾ç¡®åº¦: {precision[1]:.3f}")
    print(f"- å¬å›ç‡: {recall[1]:.3f}")
    print(f"- F1åˆ†æ•°: {f1[1]:.3f}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_bin_test_enc, predictions)
    print("\næ··æ·†çŸ©é˜µ:")
    print("          é¢„æµ‹")
    print("å®é™…    è‰¯æ€§  æ¶æ„")
    print(f"è‰¯æ€§    {cm[0,0]:4d}  {cm[0,1]:4d}")
    print(f"æ¶æ„    {cm[1,0]:4d}  {cm[1,1]:4d}")
    
    # æ›´æ–°resultså­—å…¸
    results.update({
        'detailed_precision': precision.tolist(),
        'detailed_recall': recall.tolist(),
        'detailed_f1': f1.tolist(),
        'confusion_matrix': cm.tolist(),
        'data_balance_ratio': float(imbalance_ratio),
        'test_benign_ratio': float(test_benign_ratio),
        'test_malicious_ratio': float(test_malicious_ratio)
    })
    
    # 11. ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    model.save_models()
    
    # 12. ç”Ÿæˆå¯è§†åŒ–
    print("\nğŸ“ˆ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
    generate_training_report(ae_losses, results)
    
    return model, results

def generate_training_report(ae_losses, results):
    """Generate training report with visualizations"""
    print("\nğŸ“ˆ Generating training report...")
    
    # Create save directory
    save_dir = Path('./reports')
    save_dir.mkdir(exist_ok=True)
    
    # Set style
    plt.style.use('default')
    plt.rcParams['figure.figsize'] = [15, 10]
    plt.rcParams['font.size'] = 10
    plt.rcParams['axes.grid'] = True
    plt.rcParams['grid.alpha'] = 0.3
    
    fig = plt.figure(constrained_layout=True)
    gs = fig.add_gridspec(2, 2)
    
    # Plot autoencoder loss curves
    ax1 = fig.add_subplot(gs[0, 0])
    train_losses, val_losses = ae_losses
    epochs = range(1, len(train_losses) + 1)
    
    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2, alpha=0.7)
    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2, alpha=0.7)
    ax1.set_title('Autoencoder Training Progress', pad=10)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot confusion matrix
    ax2 = fig.add_subplot(gs[0, 1])
    cm = np.array(results['confusion_matrix'])
    im = ax2.imshow(cm, interpolation='nearest', cmap='Blues')
    ax2.set_title('Confusion Matrix')
    
    # Add colorbar
    plt.colorbar(im, ax=ax2)
    
    # Add labels
    classes = ['Benign', 'Malicious']
    tick_marks = np.arange(len(classes))
    ax2.set_xticks(tick_marks)
    ax2.set_yticks(tick_marks)
    ax2.set_xticklabels(classes)
    ax2.set_yticklabels(classes)
    
    # Add text annotations
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        ax2.text(j, i, format(cm[i, j], 'd'),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    
    ax2.set_ylabel('True label')
    ax2.set_xlabel('Predicted label')
    
    # Plot detection stage distribution
    ax3 = fig.add_subplot(gs[1, 0])
    stages = ['Stage 1', 'Stage 2', 'Stage 3']
    stage_counts = [results['stage_stats']['stage1'],
                   results['stage_stats']['stage2'],
                   results['stage_stats']['stage3']]
    
    colors = ['#3498db', '#e67e22', '#9b59b6']
    bars = ax3.bar(stages, stage_counts, color=colors)
    ax3.set_title('Detection Stage Distribution')
    ax3.set_ylabel('Number of Samples')
    
    # Add value labels on top of bars
    for bar in bars:
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}',
                ha='center', va='bottom')
    
    # Plot performance metrics
    ax4 = fig.add_subplot(gs[1, 1])
    metrics = ['Precision', 'Recall', 'F1-Score']
    scores = [results['precision'], results['recall'], results['f1']]
    colors = ['#2ecc71', '#e74c3c', '#f1c40f']
    
    bars = ax4.bar(metrics, scores, color=colors)
    ax4.set_title('Model Performance Metrics')
    ax4.set_ylim(0, 1)
    
    # Add value labels on top of bars
    for bar in bars:
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}',
                ha='center', va='bottom')
    
    # Save the figure
    plt.savefig(save_dir / 'training_report.png', dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ“ Training report saved to {save_dir}/training_report.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›¡ï¸  å¢å¼ºé›¶ä¿¡ä»»IoTå…¥ä¾µæ£€æµ‹ç³»ç»Ÿè®­ç»ƒ")
    print("=" * 50)
    
    # æ£€æŸ¥PyTorchå’ŒCUDA
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
    print()
    
    try:
        # å¼€å§‹è®­ç»ƒ
        model, results = train_enhanced_model()
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ç»ˆç²¾ç¡®åº¦: {results['precision']:.4f}")
        print(f"æœ€ç»ˆå¬å›ç‡: {results['recall']:.4f}")
        print(f"æœ€ç»ˆF1åˆ†æ•°: {results['f1']:.4f}")
        
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("- improved_zero_trust_ids/enhanced_ids_*.pth (æ¨¡å‹æ–‡ä»¶)")
        print("- improved_zero_trust_ids/training_report.png (è®­ç»ƒæŠ¥å‘Š)")
        print("- improved_zero_trust_ids/training_results.txt (è¯¦ç»†ç»“æœ)")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 